{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The evolution of the language in films"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "from pyspark.sql import *\n",
    "%matplotlib inline\n",
    "\n",
    "#spark = SparkSession.builder.getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Subtitle analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preprocessing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the subtitle analysis, we will do some preprocessing steps in order to transform the subtitle texts to a more suitable format and remove undesirable parts. \n",
    "\n",
    "As previously mentioned in the parsing section, sentences are stored as lists of strings. For instance the sentence \"You're a lovely person.\" would be represented by the following list:\n",
    "`[\"You\", \"'re\", \"a\", \"lovely\", \"person\", \".\"]`.\n",
    "\n",
    "\n",
    "There are a few types of words that we do not desire to be part of the analysis, we want to remove common words that do not add any value or meaning to the text. One such category is stop words (https://en.wikipedia.org/wiki/Stop_words). \n",
    "\n",
    "We also do not care about the punctuations (https://en.wikipedia.org/wiki/Punctuation) hence we will remove those as well.  \n",
    "\n",
    "Finally, we also want to transform each word into its \"base\" form. For instance, the words take, took and taken should be treated as a single word in the analysis and not as three different ones. We will use Lemmatisation (https://en.wikipedia.org/wiki/Lemmatisation) in order to turn \"took\" and \"taken\" into their verb base form which is \"take\". However, we do not only want to lemmatize words but also other cases such as transforming plural words into singular and remove the -ing part of words (walking -> walk) etc.\n",
    "\n",
    "Note from the above example sentence that contracted words (https://en.wikipedia.org/wiki/Wikipedia:List_of_English_contractions) are represented as two words in the sentence list. This makes sence when calculating the length of each sentence since a contracted word is actually two words. However, the part on the right hande side of the apostrophe in the contracted word does not add any value to our analysis. Therefore we will drop any word which starts with an apostrophe. \n",
    "\n",
    "Finally, we also want to transform each word into lower case. We want the words \"Take\" and \"take\" to be treated as the same word.\n",
    "\n",
    "To summerize, we are doing the folloing preprocessing steps of our subtitle data:\n",
    "\n",
    "1. Transform each word into lower case \n",
    "2. Remove stop words\n",
    "3. Remove punctuation \n",
    "4. Lemmatize words\n",
    "5. Remove words which starts with an apostrophe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitle_df = spark.read.parquet(\"./subtitle_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Let's convert each word into its lower case representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitle_df = subtitle_df.select('_id', F.lower(F.col('w')).alias('word'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use NLTK (https://www.nltk.org/) for point 2 in the above list. NLTK provides predefined lists of stop words for several different languages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=set(stopwords.words('english'))\n",
    "subtitle_df = subtitle_df\\\n",
    "                    .filter(subtitle_df.word.isin(stop_words) == False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For point 3, we can use the built in puncuation list of the string class in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_list=list(string.punctuation)\n",
    "subtitle_df = subtitle_df\\\n",
    "                    .filter(subtitle_df.word.isin(punctuation_list) == False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK library also provies a lemmatizer API which we will use to solve point 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatize = F.udf(lambda x: lemmatizer.lemmatize(x, 'v'), StringType())\n",
    "subtitle_df = subtitle_df.select('_id', lemmatize(F.col('word')).alias('word'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
