{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The evolution of the language in films"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rinjac/anaconda3/envs/ada/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/rinjac/anaconda3/envs/ada/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# pandas udfs\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "from pyspark.sql import *\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "#spark = SparkSession.builder.getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Extraction of data\n",
    "\n",
    "Each subtitle in the OpenSubtitle database is stored in a seperate XML-file. In order to analyse the data and calculate our descriptive statistics we need to convert it to a format more appropriate for Spark/Pandas. Instead of having ~180000 different XML-files, we would like to have the data stored in a Spark dataframe.\n",
    "\n",
    "In order to process the dataset using Spark, we will use the spark-xml package from databricks (https://github.com/databricks/spark-xml).\n",
    "\n",
    "When using spark-xml, you can either let spark-xml infer the schema of the XML file or you can specify it yourself. Delegating the job of creating the schema to spark-xml is convenient however it is very expensive which we experienced through first hand experience. In order to speed up our input pipeline, we will specify or own static xml schema below. When specifying the schema yourself, spark-xml will not try to infer the schema which gives a significant performance boost when reading large numbers of XML-files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will extract 2 seperate dataframes, therefore we will need two different XML schemas. One schema will be used to extract the movie meta data contained in each subtitle file and the other schema will be used to extract the actual subtitle text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1.1 meta data schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema for the source XML-tag\n",
    "sourceSchema = StructType([\\\n",
    "                            StructField('genre', StringType()),\n",
    "                            StructField('year', StringType()),\n",
    "                          ])\n",
    "\n",
    "# Schema for the source XML-tag\n",
    "subtitleSchema = StructType([\\\n",
    "                            StructField('duration', StringType())\n",
    "                          ])\n",
    "\n",
    "# Schema for the conversion XML-tag\n",
    "conversionSchema =  StructType([\\\n",
    "                                StructField('sentences', StringType()),\n",
    "                                StructField('tokens', StringType())\n",
    "                              ])\n",
    "\n",
    "# Schema for the meta XML-tag\n",
    "mSchema = StructType([\\\n",
    "                        StructField('source', sourceSchema),\n",
    "                        StructField('conversion', conversionSchema),\n",
    "                        StructField('subtitle', subtitleSchema),\n",
    "                    ])\n",
    "\n",
    "# Root schema\n",
    "metaSchema = StructType([\n",
    "    StructField('_id', IntegerType()),\n",
    "    StructField('meta', mSchema)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1.1 subtitle data schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The schema will produce a dataframe with two columns. One column will contain the movie id and a column **s** which is of type Array. The elements of the Array in column **s** holds elements which also are arrays. Each nested array represents a sentence and each element of the nested Arrays are the words of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema for the w (word) XML-tag\n",
    "wordSchema = ArrayType(StructType([\\\n",
    "                                StructField('_VALUE', StringType())\\\n",
    "                               ]))\n",
    "\n",
    "# Schema for the s (sentence) XML-tag\n",
    "sentenceSchema = StructType([StructField('w', wordSchema)])\n",
    "\n",
    "# Root schema\n",
    "sentenceSchema = StructType([\n",
    "    StructField('_id', IntegerType()),\n",
    "    StructField('s', ArrayType(sentenceSchema))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Parsing of xml-data\n",
    "\n",
    "The OpenSubtitle dataset uses the following folder structure:\n",
    "\n",
    "**opensubtitle/OpenSubtitles2018/xml/ { language } / { year } / { imdb_id } / { openSubtitle_id }.xml.gz**\n",
    "\n",
    "* year is the release year of the movie\n",
    "* langauge is the language of the movie\n",
    "* imdb_id: id of the movie in the IMDB database\n",
    "* openSubtitle_id: unique identifier in the opensubtitle database\n",
    "\n",
    "With the help of regular expression we can load every single subtitle file at once using spark. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.2.1 meta data dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The meta data we extract for each movie is the following:**\n",
    "\n",
    "* **_id**: the openSubtitle id - **type**: string\n",
    "* **year**: the release year of the movie - **type**: numeric\n",
    "* **sentences**: the total amount of sentences in the movie - **type**: numeric\n",
    "* **words**: the total amount of words in the movie (not unique words) - **type**: numeric\n",
    "* **duration**:: the duration of the movie - **type**: string\n",
    "* **imdb_id**: id of the movie in the IMDB database - **type**: string\n",
    "\n",
    "The imdb_id is actually not contained in the XML-file of each movie but the parent directory of each XML subtitle file is named using the imdb identifier. Therefore, we extract the imdb_id from the file path of each subtitle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads all subtitles files in english and parses the input files with the schema for meta data.\n",
    "df_metadata = sqlContext.read.format('com.databricks.spark.xml')\\\n",
    "    .options(rowTag='document', \n",
    "             samplingRatio=0.0)\\\n",
    "    .load('hdfs:///datasets/opensubtitle/OpenSubtitles2018/xml/en/[1-2][0-9][0-9][0-9]/**/*.xml.gz', \n",
    "          schema=metaSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movies_meta = df_metadata.select(\\\n",
    "               F.col(\"_id\"),\\\n",
    "               F.col(\"meta.source.genre\").alias(\"genre\"),\\\n",
    "               F.col(\"meta.source.year\").alias(\"year\"),\\\n",
    "               F.col(\"meta.conversion.sentences\").alias(\"sentences\"),\\\n",
    "               F.col(\"meta.conversion.tokens\").alias(\"words\"),\\\n",
    "               F.col(\"meta.subtitle.duration\").alias(\"duration\"))  \n",
    "\n",
    "movies_meta = movies.withColumn(\"imdb_id\",F.split(F.input_file_name(), \"/\").getItem(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_meta.write.mode(\"overwrite\").parquet('movie_data_with_imdb_id.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.2.1 subtitle dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subtitle dataframe has the following structure:\n",
    "\n",
    "* **_id:** the openSubtitle id - **type**: string\n",
    "* **sentence:** a sentence belonging to the movie identified by the **_id** field - **type**: List of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads all subtitles files in english and parses the input files with the schema for subtitle data.\n",
    "df_subtitle = sqlContext.read.format('com.databricks.spark.xml')\\\n",
    "    .options(rowTag='document', \n",
    "             samplingRatio=0.0)\\\n",
    "    .load('hdfs:///datasets/opensubtitle/OpenSubtitles2018/xml/en/[1-2][0-9][0-9][0-9]/**/*.xml.gz', \n",
    "          schema=sentenceSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tag s.w in the nested XML-structure holds an array of sentences for each movie.\n",
    "# We want each movie to have its own entry in the dataframe, we use the explode function to create one row\n",
    "# in the dataframe for each nested array contained in the s.w tag\n",
    "movies_subtitle = df.select('_id',\n",
    "                            F.explode(F.col('s.w')).alias('sentence'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_subtitle.write.mode(\"overwrite\").parquet('movies_subtitle.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Enriching the dataset\n",
    "\n",
    "With the help of the imdb_id, we can enrich our dataset with one or several of IMDBs public datasets (https://www.imdb.com/interfaces/). We are interested in the imdb rating of each movie, let's add it to our existing movies dataframe by joining it with the ratings dataset of IMDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----+---------+-----+------------+-------+\n",
      "|    _id|               genre|year|sentences|words|    duration|imdb_id|\n",
      "+-------+--------------------+----+---------+-----+------------+-------+\n",
      "|7018237|Comedy,Drama,Romance|1947|     2768|20350|01:55:25,969|  39477|\n",
      "|3528271| Crime,Drama,Fantasy|1993|     1557|10885|01:56:41,202| 107665|\n",
      "|4655639|Animation,Comedy,...|1935|      142|  457|00:15:15,010|  26706|\n",
      "|4336532|Adventure,Crime,D...|1964|      906| 6723|00:50:21,667| 583992|\n",
      "|3452622|Crime,Mystery,Thr...|1922|      630| 7016|03:54:14,811|  13086|\n",
      "+-------+--------------------+----+---------+-----+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies = spark.read.parquet(\"./movie_data_with_imdb_id.parquet\")\n",
    "movies.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------+\n",
      "|   tconst|averageRating|numVotes|\n",
      "+---------+-------------+--------+\n",
      "|tt0000001|          5.8|    1437|\n",
      "|tt0000002|          6.3|     171|\n",
      "|tt0000003|          6.6|    1037|\n",
      "|tt0000004|          6.4|     102|\n",
      "|tt0000005|          6.2|    1734|\n",
      "+---------+-------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imdb_ratings = spark.read.format(\"csv\")\\\n",
    "                        .option(\"header\", \"true\")\\\n",
    "                        .option(\"sep\", \"\\t\").load(\"title.ratings.tsv\")\n",
    "imdb_ratings.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some unknown reason, the imdb id's in the imdb_ratings dataframe are prefixed with the string \"tt\". We need to get rid of the prefix in order to join the imdb_ratings dataframe with our movies dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_prefix = F.udf(lambda x: x[2:], StringType())\n",
    "imdb_ratings = imdb_ratings.select('averageRating', \n",
    "                                   'numVotes', \n",
    "                                   remove_prefix(F.col('tconst')).alias('_id'))\n",
    "# we do a left outer join since we still want to keep movies from our original dataset that \n",
    "# does not match any movies in the imdb dataset.\n",
    "movies = movies.join(imdb_ratings, movies.imdb_id == imdb_ratings._id, \"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.write.mode(\"overwrite\").parquet('movie_data_enriched.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Meta data dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before working with our data, let's get to know it. We will start by getting the movie data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 177036 movies in the dataset.\n"
     ]
    }
   ],
   "source": [
    "movies = spark.read.parquet(\"./movie_data.parquet\")\n",
    "print(\"There are \" + str(movies.count()) + \" movies in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What information do we have for every movie?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(_id,IntegerType,true),StructField(genre,StringType,true),StructField(year,StringType,true),StructField(sentences,StringType,true),StructField(words,StringType,true),StructField(duration,StringType,true)))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- genre: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- sentences: integer (nullable = true)\n",
      " |-- words: integer (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert to DF format\n",
    "movies = movies.toDF('id', 'genre', 'year', 'sentences', 'words', 'duration')\n",
    "\n",
    "# number of words and sentences are ints \n",
    "movies = movies.withColumn(\"words\", movies[\"words\"].cast(\"int\"))\n",
    "movies = movies.withColumn(\"sentences\", movies[\"sentences\"].cast(\"int\"))\n",
    "\n",
    "# convert duration to minutes\n",
    "@udf('string')\n",
    "def convert_duration(duration):\n",
    "    try:\n",
    "        return str(int(duration[:2]) * 60 + int(duration[3:5])) \n",
    "    except: \n",
    "        return \"0\"\n",
    "\n",
    "movies = movies.withColumn(\"duration\", convert_duration(movies.duration))\n",
    "movies = movies.withColumn(\"duration\", movies[\"duration\"].cast(\"int\"))\n",
    "\n",
    "movies.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = movies.filter(movies.duration > 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74673"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Number of movies by year\n",
    "\n",
    "So how many movies are there for each year? We presume there will be more movies in recent years than 1950. \n",
    "\n",
    "Firstly, we check the range of the years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|             year|\n",
      "+-------+-----------------+\n",
      "|  count|            74415|\n",
      "|   mean|1995.367452798495|\n",
      "| stddev|21.92278049187245|\n",
      "|    min|             1906|\n",
      "|    max|             2017|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.describe('year').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's draw a chart showing exactly how many movies were there for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get movie counts per year\n",
    "movies_per_year = movies.groupby('year').count()\n",
    "\n",
    "# convert to Pandas for plotting\n",
    "ydf = movies_per_year.toPandas()\n",
    "\n",
    "# sort the films by year\n",
    "ydf = ydf.sort_values(by='year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to clean our data of null values and inconsistent entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning we have 74673 films. \n",
      "After cleaning we have 74415 films. \n",
      "Only 0.00% of our dataset was invalid/inconsistent.\n"
     ]
    }
   ],
   "source": [
    "n_before = sum(ydf['count'])\n",
    "print(\"Before cleaning we have \" + str(n_before) + \" films. \")\n",
    "\n",
    "# convert year to int the 'year' value\n",
    "ydf['year'] = pd.to_numeric(ydf['year'], errors='coerce', downcast='integer')\n",
    "\n",
    "# remove films with \"Nan\" as year value\n",
    "# since we cannot really use them\n",
    "ydf = ydf.dropna()\n",
    "ydf = ydf.reset_index(drop=True)\n",
    "ydf['year'] = ydf['year'].astype('int')\n",
    "\n",
    "n_after = sum(ydf['count'])\n",
    "print(\"After cleaning we have \" + str(n_after) + \" films. \")\n",
    "print(\"Only %.2f%% of our dataset was invalid/inconsistent.\" % (1 - (n_after/n_before)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the number of films by year, we will bin them to make the visualization clearer.\n",
    "\n",
    "Considering we have movies from year 1906 to 2020, we are going to make 12 bins for each decade from 1900's to 2010's (We are going to put year 2020 in 2010's, it will not change much because we have only  6 films for 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(ydf['year'])\n",
    "labels = []\n",
    "counts = []\n",
    "\n",
    "for decade in range(1900, 2020, 10):\n",
    "    # create bin label\n",
    "    label = str(decade) + \"s\"\n",
    "    labels.append(label)\n",
    "    \n",
    "    # calculate count for the bin\n",
    "    count = 0\n",
    "    for year in range(decade, decade+10):\n",
    "        if year in years:\n",
    "            count += int(ydf.loc[ydf['year'] == year]['count'])\n",
    "    counts.append(count)\n",
    "\n",
    "# add the 2020 to the 2010s decade\n",
    "counts[11] += int(ydf.loc[ydf['year'] == 2010]['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(labels, counts)\n",
    "plt.title('Nuber of films per decade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we have more films in the recent decades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Median number of sentences by year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our project is about evolution of language used in films, so let's have a peek at how and did it change over time. We will start by showing the median number of sentences for each year. We use median instead of mean since it's a more robust measure so in the case we have some invalid data (like unfinished movie subtitles with just a few sentences) it will behave better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get median number of sentences per year\n",
    "percentile = F.expr('percentile_approx(sentences, 0.5)')\n",
    "sdf_ = movies.groupby('year').agg(percentile.alias('n_sent_median'))\n",
    "\n",
    "# convert to Pandas for plotting\n",
    "sdf = sdf_.toPandas()\n",
    "\n",
    "# sort the films by year\n",
    "sdf = sdf.sort_values(by='year')\n",
    "\n",
    "# convert year to int the 'year' value\n",
    "sdf['year'] = pd.to_numeric(sdf['year'], errors='coerce', downcast='integer')\n",
    "\n",
    "# remove films with \"Nan\" as year value\n",
    "# since we cannot really use them\n",
    "sdf = sdf.dropna()\n",
    "sdf = sdf.reset_index(drop=True)\n",
    "sdf['year'] = sdf['year'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now visualize the median number of sentences by decade. For each decade, we will take the average of medians for that decade. We will ignore year 2020 (for all decades to have exactly 10 years)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(sdf['year'])\n",
    "labels = []\n",
    "medians = []\n",
    "\n",
    "for decade in range(1900, 2020, 10):\n",
    "    # create bin label\n",
    "    label = str(decade) + \"s\"\n",
    "    labels.append(label)\n",
    "    \n",
    "    # calculate count for the bin\n",
    "    median = 0\n",
    "    count = 0\n",
    "    for year in range(decade, decade+10):\n",
    "        if year in years:\n",
    "            median += int(sdf.loc[sdf['year'] == year]['n_sent_median'])\n",
    "            count += 1\n",
    "    medians.append(median / count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(labels, medians)\n",
    "plt.title('Median number of sentences per decade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this graph we can see a lot of interesting things. Before 1930s, in the era of the silent films, there is a low number of sentences for each movie. In the 40s, the first decade where pretty much every film had sound, we have the highest number of sentences. As we go to our time, we have fewer and fewer sentences by each film. \n",
    "\n",
    "This gives credit to the idea that movies are getting more \"simpler\" by years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Median number of words in sentences by year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already checked sentences, but they can hide something. The number of words is also important. A movie can have many sentences but most of them can be very short. So let's check the median number of words per year and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get median number of words per year\n",
    "percentile = F.expr('percentile_approx(words, 0.5)')\n",
    "wdf_ = movies.groupby('year').agg(percentile.alias('n_word_median'))\n",
    "\n",
    "# convert to Pandas for plotting\n",
    "wdf = wdf_.toPandas()\n",
    "\n",
    "# sort the films by year\n",
    "wdf = wdf.sort_values(by='year')\n",
    "\n",
    "# convert year to int the 'year' value\n",
    "wdf['year'] = pd.to_numeric(wdf['year'], errors='coerce', downcast='integer')\n",
    "\n",
    "# remove films with \"Nan\" as year value\n",
    "# since we cannot really use them\n",
    "wdf = wdf.dropna()\n",
    "wdf = wdf.reset_index(drop=True)\n",
    "wdf['year'] = wdf['year'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(wdf['year'])\n",
    "labels = []\n",
    "medians = []\n",
    "\n",
    "for decade in range(1900, 2020, 10):\n",
    "    # create bin label\n",
    "    label = str(decade) + \"s\"\n",
    "    labels.append(label)\n",
    "    \n",
    "    # calculate count for the bin\n",
    "    median = 0\n",
    "    count = 0\n",
    "    for year in range(decade, decade+10):\n",
    "        if year in years:\n",
    "            median += int(wdf.loc[wdf['year'] == year]['n_word_median'])\n",
    "            count += 1\n",
    "    medians.append(median / count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(labels, medians)\n",
    "plt.title('Median number of words per decade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph seems very similar to the sentences graph in the previous section. So we can assume that newer movies have less sentences, but that number of words for each sentence did not really change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 Median film duration by year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the number of sentences per year we noticed that there is a smaller number of sentences in films in recent decades. However, could it be just because films were longer in the past? \n",
    "\n",
    "We will check the median film duration for every year. Again, we will visualize it in decades, taking the average of the median for each year of the decade. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get median duration per year\n",
    "percentile = F.expr('percentile_approx(duration, 0.5)')\n",
    "ddf_ = movies.groupby('year').agg(percentile.alias('duration_median'))\n",
    "\n",
    "# convert to Pandas for plotting\n",
    "ddf = ddf_.toPandas()\n",
    "\n",
    "# sort the films by year\n",
    "ddf = ddf.sort_values(by='year')\n",
    "\n",
    "# convert year to int the 'year' value\n",
    "ddf['year'] = pd.to_numeric(ddf['year'], errors='coerce', downcast='integer')\n",
    "\n",
    "# remove films with \"Nan\" as year value\n",
    "# since we cannot really use them\n",
    "ddf = ddf.dropna()\n",
    "ddf = ddf.reset_index(drop=True)\n",
    "ddf['year'] = ddf['year'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(ddf['year'])\n",
    "labels = []\n",
    "medians = []\n",
    "\n",
    "for decade in range(1900, 2020, 10):\n",
    "    # create bin label\n",
    "    label = str(decade) + \"s\"\n",
    "    labels.append(label)\n",
    "    \n",
    "    # calculate count for the bin\n",
    "    median = 0\n",
    "    count = 0\n",
    "    for year in range(decade, decade+10):\n",
    "        if year in years:\n",
    "            median += int(ddf.loc[wdf['year'] == year]['duration_median'])\n",
    "            count += 1\n",
    "    medians.append(median / count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(labels, medians)\n",
    "plt.title('Median duration per decade (in minutes)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Subtitle dataframe\n",
    "\n",
    "Let's start by importing the dataframe and printing its schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitle_df = spark.read.parquet(\"./sentences_until_1999_v2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(_id,IntegerType,true),StructField(sentence,ArrayType(StructType(List(StructField(_VALUE,StringType,true))),true),true),StructField(sentence_length,IntegerType,true)))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtitle_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the total amount of sentences we have in our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of sentences: 48996903\n"
     ]
    }
   ],
   "source": [
    "print(\"Total amount of sentences: {}\".format(subtitle_df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue by counting the total amount of words. Since each sentence is saved as an array of words, we can easily calculate the sentence length by computing the length of each sentence array and if we later sum over this column we will get the total amount of words in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column with sentence length\n",
    "subtitle_df = subtitle_df.withColumn('sentence_length',F.size(F.col('sentence')))\n",
    "\n",
    "# Sum all sentence lenegths to get total amount of words\n",
    "total_amount_of_words = subtitle_df.groupBy().sum().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of words: 5914305524454620\n"
     ]
    }
   ],
   "source": [
    "print(\"Total amount of words: {}\".format(total_amount_of_words[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be observed in the above printouts, we are dealing with a lot of text. However, it's important to note that not all words will be used in the analysis. We will elaborate more on this below in the **Preprocessing** section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the text analysis, we will do some preprocessing steps in order to transform the subtitle texts to a more suitable format and remove undesirable parts. \n",
    "\n",
    "As previously mentioned in the parsing section, sentences are stored as lists of strings. For instance the sentence \"You're a lovely person.\" would be represented by the following list:\n",
    "`[\"You\", \"'re\", \"a\", \"lovely\", \"person\", \".\"]`.\n",
    "\n",
    "\n",
    "There are a few types of words that we do not desire to be part of the analysis, we want to remove common words that do not add any value or meaning to the text. One such category is stop words (https://en.wikipedia.org/wiki/Stop_words). \n",
    "\n",
    "We also do not care about the punctuations (https://en.wikipedia.org/wiki/Punctuation) hence we will remove those as well.  \n",
    "\n",
    "Finally, we also want to transform each word into its \"base\" form. For instance, the words take, took and taken should be treated as a single word in the analysis and not as three different ones. We will use Lemmatisation (https://en.wikipedia.org/wiki/Lemmatisation) in order to turn \"took\" and \"taken\" into their verb base form which is \"take\". However, we do not only want to lemmatize words but also other cases such as transforming plural words into singular and remove the -ing part of words (walking -> walk) etc.\n",
    "\n",
    "Note from the above example sentence that contracted words (https://en.wikipedia.org/wiki/Wikipedia:List_of_English_contractions) are represented as two words in the sentence list. This makes sence when calculating the length of each sentence since a contracted word is actually two words. However, the part on the right hande side of the apostrophe in the contracted word does not add any value to our analysis. Therefore we will drop any word which starts with an apostrophe. \n",
    "\n",
    "Finally, we also want to transform each word into lower case. We want the words \"Take\" and \"take\" to be treated as the same word.\n",
    "\n",
    "To summerize, we are doing the folloing preprocessing steps of our subtitle data:\n",
    "\n",
    "1. Transform each word into lower case \n",
    "2. Remove stop words\n",
    "3. Remove punctuation \n",
    "4. Lemmatize words\n",
    "5. Remove words which starts with an apostrophe\n",
    "\n",
    "Let's perform the 5 steps described above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o93.parquet.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.io.IOException: Failed to connect to /128.179.251.7:56427\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$downloadClient(NettyRpcEnv.scala:368)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply$mcV$sp(NettyRpcEnv.scala:336)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply(NettyRpcEnv.scala:335)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply(NettyRpcEnv.scala:335)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1415)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.openChannel(NettyRpcEnv.scala:339)\n\tat org.apache.spark.util.Utils$.doFetchFile(Utils.scala:665)\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:489)\n\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:755)\n\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:747)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:99)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:747)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:312)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\tSuppressed: java.lang.NullPointerException\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1423)\n\t\t... 17 more\nCaused by: io.netty.channel.AbstractChannel$AnnotatedNoRouteToHostException: No route to host: /128.179.251.7:56427\n\tat sun.nio.ch.Net.connect0(Native Method)\n\tat sun.nio.ch.Net.connect(Net.java:454)\n\tat sun.nio.ch.Net.connect(Net.java:446)\n\tat sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648)\n\tat io.netty.util.internal.SocketUtils$3.run(SocketUtils.java:83)\n\tat io.netty.util.internal.SocketUtils$3.run(SocketUtils.java:80)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.connect(SocketUtils.java:80)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doConnect(NioSocketChannel.java:308)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.connect(AbstractNioChannel.java:254)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.connect(DefaultChannelPipeline.java:1291)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:545)\n\tat io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:530)\n\tat io.netty.channel.ChannelOutboundHandlerAdapter.connect(ChannelOutboundHandlerAdapter.java:47)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:545)\n\tat io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:530)\n\tat io.netty.channel.ChannelDuplexHandler.connect(ChannelDuplexHandler.java:50)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:545)\n\tat io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:530)\n\tat io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:512)\n\tat io.netty.channel.DefaultChannelPipeline.connect(DefaultChannelPipeline.java:994)\n\tat io.netty.channel.AbstractChannel.connect(AbstractChannel.java:259)\n\tat io.netty.bootstrap.Bootstrap$3.run(Bootstrap.java:252)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n\t... 1 more\nCaused by: java.net.NoRouteToHostException: No route to host\n\t... 29 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:611)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:241)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\n\tat scala.Option.orElse(Option.scala:289)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:202)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:622)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: Failed to connect to /128.179.251.7:56427\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$downloadClient(NettyRpcEnv.scala:368)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply$mcV$sp(NettyRpcEnv.scala:336)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply(NettyRpcEnv.scala:335)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply(NettyRpcEnv.scala:335)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1415)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.openChannel(NettyRpcEnv.scala:339)\n\tat org.apache.spark.util.Utils$.doFetchFile(Utils.scala:665)\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:489)\n\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:755)\n\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:747)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:99)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:747)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:312)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\tSuppressed: java.lang.NullPointerException\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1423)\n\t\t... 17 more\nCaused by: io.netty.channel.AbstractChannel$AnnotatedNoRouteToHostException: No route to host: /128.179.251.7:56427\n\tat sun.nio.ch.Net.connect0(Native Method)\n\tat sun.nio.ch.Net.connect(Net.java:454)\n\tat sun.nio.ch.Net.connect(Net.java:446)\n\tat sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648)\n\tat io.netty.util.internal.SocketUtils$3.run(SocketUtils.java:83)\n\tat io.netty.util.internal.SocketUtils$3.run(SocketUtils.java:80)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.connect(SocketUtils.java:80)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doConnect(NioSocketChannel.java:308)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.connect(AbstractNioChannel.java:254)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.connect(DefaultChannelPipeline.java:1291)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:545)\n\tat io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:530)\n\tat io.netty.channel.ChannelOutboundHandlerAdapter.connect(ChannelOutboundHandlerAdapter.java:47)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:545)\n\tat io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:530)\n\tat io.netty.channel.ChannelDuplexHandler.connect(ChannelDuplexHandler.java:50)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:545)\n\tat io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:530)\n\tat io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:512)\n\tat io.netty.channel.DefaultChannelPipeline.connect(DefaultChannelPipeline.java:994)\n\tat io.netty.channel.AbstractChannel.connect(AbstractChannel.java:259)\n\tat io.netty.bootstrap.Bootstrap$3.run(Bootstrap.java:252)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n\t... 1 more\nCaused by: java.net.NoRouteToHostException: No route to host\n\t... 29 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9eea92bb44bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msubtitle_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./sentences_until_1999_v2.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msubtitle_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-2.3.2-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \"\"\"\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-2.3.2-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-2.3.2-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-2.3.2-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o93.parquet.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.io.IOException: Failed to connect to /128.179.251.7:56427\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$downloadClient(NettyRpcEnv.scala:368)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply$mcV$sp(NettyRpcEnv.scala:336)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply(NettyRpcEnv.scala:335)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply(NettyRpcEnv.scala:335)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1415)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.openChannel(NettyRpcEnv.scala:339)\n\tat org.apache.spark.util.Utils$.doFetchFile(Utils.scala:665)\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:489)\n\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:755)\n\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:747)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:99)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:747)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:312)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\tSuppressed: java.lang.NullPointerException\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1423)\n\t\t... 17 more\nCaused by: io.netty.channel.AbstractChannel$AnnotatedNoRouteToHostException: No route to host: /128.179.251.7:56427\n\tat sun.nio.ch.Net.connect0(Native Method)\n\tat sun.nio.ch.Net.connect(Net.java:454)\n\tat sun.nio.ch.Net.connect(Net.java:446)\n\tat sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648)\n\tat io.netty.util.internal.SocketUtils$3.run(SocketUtils.java:83)\n\tat io.netty.util.internal.SocketUtils$3.run(SocketUtils.java:80)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.connect(SocketUtils.java:80)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doConnect(NioSocketChannel.java:308)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.connect(AbstractNioChannel.java:254)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.connect(DefaultChannelPipeline.java:1291)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:545)\n\tat io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:530)\n\tat io.netty.channel.ChannelOutboundHandlerAdapter.connect(ChannelOutboundHandlerAdapter.java:47)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:545)\n\tat io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:530)\n\tat io.netty.channel.ChannelDuplexHandler.connect(ChannelDuplexHandler.java:50)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:545)\n\tat io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:530)\n\tat io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:512)\n\tat io.netty.channel.DefaultChannelPipeline.connect(DefaultChannelPipeline.java:994)\n\tat io.netty.channel.AbstractChannel.connect(AbstractChannel.java:259)\n\tat io.netty.bootstrap.Bootstrap$3.run(Bootstrap.java:252)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n\t... 1 more\nCaused by: java.net.NoRouteToHostException: No route to host\n\t... 29 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:611)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:241)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\n\tat scala.Option.orElse(Option.scala:289)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:202)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:622)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: Failed to connect to /128.179.251.7:56427\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$downloadClient(NettyRpcEnv.scala:368)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply$mcV$sp(NettyRpcEnv.scala:336)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply(NettyRpcEnv.scala:335)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply(NettyRpcEnv.scala:335)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1415)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.openChannel(NettyRpcEnv.scala:339)\n\tat org.apache.spark.util.Utils$.doFetchFile(Utils.scala:665)\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:489)\n\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:755)\n\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:747)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:99)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:747)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:312)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\tSuppressed: java.lang.NullPointerException\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1423)\n\t\t... 17 more\nCaused by: io.netty.channel.AbstractChannel$AnnotatedNoRouteToHostException: No route to host: /128.179.251.7:56427\n\tat sun.nio.ch.Net.connect0(Native Method)\n\tat sun.nio.ch.Net.connect(Net.java:454)\n\tat sun.nio.ch.Net.connect(Net.java:446)\n\tat sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648)\n\tat io.netty.util.internal.SocketUtils$3.run(SocketUtils.java:83)\n\tat io.netty.util.internal.SocketUtils$3.run(SocketUtils.java:80)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.connect(SocketUtils.java:80)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doConnect(NioSocketChannel.java:308)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.connect(AbstractNioChannel.java:254)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.connect(DefaultChannelPipeline.java:1291)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:545)\n\tat io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:530)\n\tat io.netty.channel.ChannelOutboundHandlerAdapter.connect(ChannelOutboundHandlerAdapter.java:47)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:545)\n\tat io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:530)\n\tat io.netty.channel.ChannelDuplexHandler.connect(ChannelDuplexHandler.java:50)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:545)\n\tat io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:530)\n\tat io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:512)\n\tat io.netty.channel.DefaultChannelPipeline.connect(DefaultChannelPipeline.java:994)\n\tat io.netty.channel.AbstractChannel.connect(AbstractChannel.java:259)\n\tat io.netty.bootstrap.Bootstrap$3.run(Bootstrap.java:252)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n\t... 1 more\nCaused by: java.net.NoRouteToHostException: No route to host\n\t... 29 more\n"
     ]
    }
   ],
   "source": [
    "subtitle_df = spark.read.parquet(\"./sentences_until_1999_v2.parquet\")\n",
    "subtitle_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(1)** Let's convert each word into its lower case representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|    _id|            sentence|\n",
      "+-------+--------------------+\n",
      "|5131043|[they, 're, indep...|\n",
      "|5131043|[they, 're, affec...|\n",
      "|5131043|[they, 're, loyal...|\n",
      "|5131043|[they, 're, sagac...|\n",
      "|5131043|[they, 're, ineff...|\n",
      "|5131043|[cats, are, magic...|\n",
      "|5131043|[they, really, ar...|\n",
      "|5131043|[probably, the, m...|\n",
      "|5131043|[they, 're, also,...|\n",
      "|5131043|[they, 're, very,...|\n",
      "|5131043|[that, 's, anothe...|\n",
      "|5131043|[the, domestic, c...|\n",
      "|5131043|[within, even, th...|\n",
      "|5131043|[even, after, tho...|\n",
      "|5131043|[now, ,, scientis...|\n",
      "|5131043|[for, them, ,, th...|\n",
      "|5131043|[to, share, one, ...|\n",
      "|5131043|[perhaps, the, wr...|\n",
      "|5131043|[\", god, made, th...|\n",
      "|5131043|[today, ,, the, w...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toLowerSentence = F.udf(lambda x: list(map(lambda y: y.lower(), x)), ArrayType(StringType()))\n",
    "subtitle_df = subtitle_df.select('_id', toLowerSentence('sentence._VALUE').alias('sentence'))\n",
    "subtitle_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** We will take help from the NLTK library(https://www.nltk.org/) to filter out stop words. NLTK provides predefined lists of stop words for several different languages, we will use the list for the english language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|    _id|            sentence|\n",
      "+-------+--------------------+\n",
      "|5131043|['re, independent...|\n",
      "|5131043|['re, affectionat...|\n",
      "|5131043|['re, loyal, ;, '...|\n",
      "|5131043|['re, sagacious, ...|\n",
      "|5131043|['re, ineffable, ...|\n",
      "|5131043|    [cats, magic, .]|\n",
      "|5131043|  [really, magic, .]|\n",
      "|5131043|[probably, myster...|\n",
      "|5131043|['re, also, vicio...|\n",
      "|5131043|['re, cruel, thin...|\n",
      "|5131043|['s, another, thi...|\n",
      "|5131043|[domestic, cat, h...|\n",
      "|5131043|[within, even, de...|\n",
      "|5131043|[even, thousands,...|\n",
      "|5131043|[,, scientists, l...|\n",
      "|5131043|[,, domestic, cat...|\n",
      "|5131043|[share, one, 's, ...|\n",
      "|5131043|[perhaps, writer,...|\n",
      "|5131043|[\", god, made, ca...|\n",
      "|5131043|[today, ,, wester...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stop_words=set(stopwords.words('english'))\n",
    "wordInStopWords = F.udf(lambda x: list(filter(lambda y: y not in stop_words,x)), ArrayType(StringType()))\n",
    "subtitle_df = subtitle_df\\\n",
    "                    .select('_id', wordInStopWords('sentence').alias('sentence'))\n",
    "subtitle_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3)** To filter out punctuations, we can use the built in puncuation list contained in the string class of Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|    _id|            sentence|\n",
      "+-------+--------------------+\n",
      "|5131043|  ['re, independent]|\n",
      "|5131043| ['re, affectionate]|\n",
      "|5131043|['re, loyal, 're,...|\n",
      "|5131043|['re, sagacious, ...|\n",
      "|5131043|['re, ineffable, ...|\n",
      "|5131043|       [cats, magic]|\n",
      "|5131043|     [really, magic]|\n",
      "|5131043|[probably, myster...|\n",
      "|5131043|['re, also, vicious]|\n",
      "|5131043|['re, cruel, things]|\n",
      "|5131043|['s, another, thi...|\n",
      "|5131043|[domestic, cat, h...|\n",
      "|5131043|[within, even, de...|\n",
      "|5131043|[even, thousands,...|\n",
      "|5131043|[scientists, laym...|\n",
      "|5131043|[domestic, cat, e...|\n",
      "|5131043|[share, one, 's, ...|\n",
      "|5131043|[perhaps, writer,...|\n",
      "|5131043|[god, made, cat, ...|\n",
      "|5131043|[today, western, ...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "punctuation_list=list(string.punctuation)\n",
    "remvovePunctuation = F.udf(lambda x: list(filter(lambda y: y not in punctuation_list,x)), ArrayType(StringType()))\n",
    "subtitle_df = subtitle_df\\\n",
    "                    .select('_id', remvovePunctuation('sentence').alias('sentence'))\n",
    "subtitle_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4)** The NLTK library provies a lemmatizer API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatize = F.udf(lambda x: lemmatizer.lemmatize(x, 'v'), StringType())\n",
    "subtitle_df = subtitle_df.select('_id', lemmatize(F.col('word')).alias('word'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5)** Remove words starting with an apostrophe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|    _id|            sentence|\n",
      "+-------+--------------------+\n",
      "|5131043|       [independent]|\n",
      "|5131043|      [affectionate]|\n",
      "|5131043|  [loyal, beautiful]|\n",
      "|5131043|[sagacious, myste...|\n",
      "|5131043|[ineffable, inscr...|\n",
      "|5131043|       [cats, magic]|\n",
      "|5131043|     [really, magic]|\n",
      "|5131043|[probably, myster...|\n",
      "|5131043|     [also, vicious]|\n",
      "|5131043|     [cruel, things]|\n",
      "|5131043|[another, thing, ...|\n",
      "|5131043|[domestic, cat, h...|\n",
      "|5131043|[within, even, de...|\n",
      "|5131043|[even, thousands,...|\n",
      "|5131043|[scientists, laym...|\n",
      "|5131043|[domestic, cat, e...|\n",
      "|5131043|[share, one, life...|\n",
      "|5131043|[perhaps, writer,...|\n",
      "|5131043|[god, made, cat, ...|\n",
      "|5131043|[today, western, ...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "removeApostopheWords = F.udf(lambda x: list(filter(lambda y: y[0] != \"'\",x)), ArrayType(StringType()))\n",
    "subtitle_df = subtitle_df\\\n",
    "                    .select('_id', removeApostopheWords('sentence').alias('sentence'))\n",
    "subtitle_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look on the poportion of words that has been filtered out during the above steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2537.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 13 in stage 96.0 failed 1 times, most recent failure: Lost task 13.0 in stage 96.0 (TID 5627, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 253, in main\n    process()\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 248, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 331, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 140, in dump_stream\n    for obj in iterator:\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 320, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 76, in <lambda>\n    return lambda *a: f(*a)\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 68, in <lambda>\n    return lambda *a: g(f(*a))\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 68, in <lambda>\n    return lambda *a: g(f(*a))\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 68, in <lambda>\n    return lambda *a: g(f(*a))\n  File \"<ipython-input-98-6ca0326a5905>\", line 1, in <lambda>\nTypeError: 'NoneType' object is not iterable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3200)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3197)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3197)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 253, in main\n    process()\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 248, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 331, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 140, in dump_stream\n    for obj in iterator:\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 320, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 76, in <lambda>\n    return lambda *a: f(*a)\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 68, in <lambda>\n    return lambda *a: g(f(*a))\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 68, in <lambda>\n    return lambda *a: g(f(*a))\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 68, in <lambda>\n    return lambda *a: g(f(*a))\n  File \"<ipython-input-98-6ca0326a5905>\", line 1, in <lambda>\nTypeError: 'NoneType' object is not iterable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-ac4784c8f0a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msubtitle_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubtitle_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sentence_length_after_filtering'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtotal_amount_of_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubtitle_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark/spark-2.3.2-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \"\"\"\n\u001b[1;32m    465\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-2.3.2-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-2.3.2-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-2.3.2-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2537.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 13 in stage 96.0 failed 1 times, most recent failure: Lost task 13.0 in stage 96.0 (TID 5627, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 253, in main\n    process()\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 248, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 331, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 140, in dump_stream\n    for obj in iterator:\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 320, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 76, in <lambda>\n    return lambda *a: f(*a)\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 68, in <lambda>\n    return lambda *a: g(f(*a))\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 68, in <lambda>\n    return lambda *a: g(f(*a))\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 68, in <lambda>\n    return lambda *a: g(f(*a))\n  File \"<ipython-input-98-6ca0326a5905>\", line 1, in <lambda>\nTypeError: 'NoneType' object is not iterable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3200)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3197)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3197)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 253, in main\n    process()\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 248, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 331, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 140, in dump_stream\n    for obj in iterator:\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 320, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 76, in <lambda>\n    return lambda *a: f(*a)\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 68, in <lambda>\n    return lambda *a: g(f(*a))\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 68, in <lambda>\n    return lambda *a: g(f(*a))\n  File \"/Users/jakobsvenningsson/spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 68, in <lambda>\n    return lambda *a: g(f(*a))\n  File \"<ipython-input-98-6ca0326a5905>\", line 1, in <lambda>\nTypeError: 'NoneType' object is not iterable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "subtitle_df = subtitle_df.withColumn('sentence_length_after_filtering',F.size(F.col('sentence')))\n",
    "total_amount_of_words = subtitle_df.groupBy().sum().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total amount of words: {}\".format(total_amount_of_words[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan for milestone 3 below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Quantification of text complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will choose and implement a method for getting a measure of textual complexity. \n",
    "\n",
    "The measure we are going to use is the Dale-Challe readability forumla, as described here:\n",
    "https://en.wikipedia.org/wiki/Dale%E2%80%93Chall_readability_formula\n",
    "\n",
    "We will also try other qantification complexity ideas:\n",
    "* https://www.geeksforgeeks.org/readability-index-pythonnlp/\n",
    "* calculate the number of syllabuses in each word https://www.howmanysyllables.com/whataresyllables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Comparison of text complexity by year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to compare the complexity of the movie language for each year.\n",
    "We want to see if the complexity differs across years.\n",
    "\n",
    "To do this, we are going to calculate the text complexity measure for each film, and then take the average of complexity for each year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Comparison of text complexity by genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to compare the complexity of the movie language for each genre.\n",
    "Do some genres use more simple language than others?\n",
    "\n",
    "To do this, we are going to calculate the text complexity measure for each film, and then take the average of complexity for each genre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Correlation between text complexity of a film and its IMDB rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to search for correlation between the film text complexity and it's IMDB rating.\n",
    "\n",
    "To do this, we are going to calculate the text complexity measure for each film, and use correlation coefs and visualization to find evidence for correlation between those variables. \n",
    "\n",
    "If we find evidence for correlation, we will try some regression methods on the two variables, such as fitting a regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
